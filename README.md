# üóúÔ∏è Efficient Multi-File Compression API with Adaptive Parallelism and Resource-Aware Strategies

## Overview
This API provides a robust, high-performance solution for compressing files and folders using **Pigz** (the Parallel Implementation of Gzip). It intelligently adapts its compression strategy based on file characteristics and system resources. Whether compressing a single large file or many small files, the API optimizes the process by:

- **Dynamically Adjusting the Compression Threshold:** Automatically adapts how many files to process in parallel versus sequentially based on available system memory and file sizes.
- **Resource Usage Monitoring:** Logs and monitors CPU and disk I/O metrics before and after compression to better understand performance and optimize processing.
- **File Type-Specific Compression:** Adjusts the compression level depending on the file type (e.g., higher for text files, lower for binary files) to maximize efficiency and compression ratio.

---

## Features and Benefits

### Adaptive Compression Logic
- **Single File:** Uses all available CPU cores for fast compression.
- **Multiple Files:** 
  - Automatically decides whether to compress files sequentially (for few large files) or in parallel (for many small files) based on a dynamic threshold.
  - Prevents CPU oversubscription by distributing threads according to the number of files and system resources.

### Dynamic Threshold Adjustment
- The system dynamically adjusts the threshold (default ~100 MB) for switching between sequential and parallel processing based on available memory, ensuring optimal resource utilization even on lower-end machines.

### Resource Usage Monitoring
- **Logs CPU and Disk I/O Metrics:** Uses the `psutil` library to capture resource usage before and after compression.
- This data helps in monitoring performance and could be used to further fine-tune the compression strategy.

### File Type-Specific Compression
- **MIME-Type Detection:** Automatically adjusts the compression level depending on whether the file is text or binary.
  - **Text Files:** May use a higher compression level for better reduction.
  - **Binary Files (e.g., images, videos):** Use a lower compression level to avoid wasting CPU cycles on files that are already compressed.

### Robust Error Handling and Cleanup
- Compresses files in place, writes output as `<filename>.gz`, and verifies integrity with the `-t` flag.
- Attempts to delete the original file after successful compression; if deletion fails (e.g., for read-only files), the error is logged but the process still returns success.

---

## Installation

1. **Clone the Repository:**
```bash
git clone https://github.com/yourusername/pigz-compressor-api.git
cd pigz-compressor-api
```
2.	Install dependencies:
```bash
pip install -r requirements.txt
```
3.	Run the FastAPI server:
```bash
uvicorn app.main:app --reload
```
4.	Open the API documentation in your browser:
```
http://localhost:8000/docs
```

---

### Usage

#### API Endpoint

GET /resource-usage

Returns current resource usage metrics (CPU usage and disk I/O). Example response:
```json
{
  "cpu_percent": 12.5,
  "disk_read": 123456789,
  "disk_write": 987654321
}
```

POST /compress
- Description: Compresses a file or folder based on adaptive strategies.
- Parameters:
  - input_path (string, required): Path to the file or folder to compress.
  - compression_level (integer, optional, default=6): Base compression level (1‚Äì9). This may be adjusted automatically based on file type.
- Response:
```json
{
  "files": [
    {
      "file_path": "/path/to/original.txt.gz",
      "original_size": 123456,
      "compressed_size": 65432,
      "status": "success",
      "usage_before": { "cpu_percent": 10.0, "disk_read": 1234567, "disk_write": 7654321 },
      "usage_after": { "cpu_percent": 25.0, "disk_read": 2234567, "disk_write": 8654321 }
    }
  ]
}
```

#### Using the Streamlit Interface

Start the Streamlit app:
```bash
streamlit run app/streamlit_app.py
```
Navigate to:
```
http://localhost:8501
```

Upload a file or enter a folder path, select the compression level, and click Compress.

---

### Technical Details

### Adaptive Compression Logic
- **Single File:**  
  Compresses with full CPU power using the user‚Äëspecified (or maximum) thread count.
- **Multiple Files:**  
  Automatically decides whether to compress sequentially (for large average file sizes) or in parallel using dynamic thread allocation.

### Dynamic Threshold Adjustment
- Adjusts the processing strategy based on the average file size and available system memory. For instance, on systems with less available memory, the threshold is lowered to prevent oversubscription.

### Resource Usage Monitoring
- Captures and logs CPU and disk I/O metrics via `psutil`.
- Exposes these metrics through a dedicated endpoint (`/resource-usage`) so that a frontend (e.g., a Streamlit app) can poll and visualize resource usage in real time.

### File Type-Specific Compression
- Detects file types using `python-magic` (if installed) or Python‚Äôs built‚Äëin `mimetypes` to adjust the compression level:
  - **Text files:** Use a higher compression level for maximum size reduction.
  - **Binary files (images, videos):** Use a lower compression level to avoid unnecessary processing.

### Dynamic Thread Allocation and Selection
- **Dynamic Allocation:**  
  The backend can adjust threads per file based on real‚Äëtime CPU usage.
- **User Selection via Streamlit:**  
  The Streamlit interface lets users choose the thread count (from 1 to the maximum available), with the default set to the max CPU cores. This selection is passed to the API so that the compression strategy can be tuned as needed.

---

## Testing

### Running the Test Suite

Tests are written using pytest. To run all tests:
```bash
pytest --maxfail=1 --disable-warnings -q
```
Test Coverage
- API Endpoint Tests: Ensure that single files, folders, and edge cases (e.g., read-only files) are handled as expected.
- Utility Function Tests: Validate dynamic threshold adjustments, resource usage logging, and file type-specific compression level adjustments.
- Resource and Performance Monitoring: Tests verify that resource metrics are captured correctly and used to inform the compression process.

### Performance Benchmarking

Scenario	CPU Cores	Avg. File Size	Compression Mode	Speed Improvement
Single Large File (1 GB)	8	1 GB	Sequential	3x
Multiple Medium Files (100)	8	50 MB	Parallel	2x
Many Small Files (1000)	8	1 MB	Parallel	4x

---

### Testing and QA
- Comprehensive testing covers single files, multiple files, edge cases, error handling, and performance.
- Uses realistic scenarios to validate:
  - Compression efficiency
  - Data integrity
  - Resource utilization
  - See the QA Test Plan for detailed testing procedures.

---

Troubleshooting
  - Error: Path not found
  - Ensure the input path exists and is correctly specified.
  - High CPU Usage
  - The API intentionally uses all available CPU resources to maximize compression speed.
  - Disk I/O Bottleneck
  - For large batches of small files, ensure the storage system can handle parallel I/O.

--- 

Future Improvements
- Dynamic Threshold Adjustment: Automatically adjust the size threshold based on system resources and file patterns.
- Resource Usage Monitoring: Integrate metrics for CPU and disk usage to optimize the compression strategy further.
- File Type-Specific Compression: Adapt the compression level based on file types (e.g., text vs. binary).

--- 
### Contributing

Feel free to open issues or submit pull requests. Contributions are welcome to enhance performance, add new features, or improve the compression logic.

---
### License

This project is licensed under the MIT License.
---
### Acknowledgments

Special thanks to the Pigz developers for creating an efficient parallel compression tool, and to the FastAPI and Streamlit communities for their excellent frameworks.